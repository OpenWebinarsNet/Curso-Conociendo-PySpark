{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones en PySpark - Parte 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este notebook aprenderemos algunas funciones avanzadas para optimizar el rendimiento de Spark, para imputar valores faltantes o a crear funciones definidas por el usuario (UDF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import broadcast\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crea la sesión de SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear el DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp = [(1, \"AAA\", \"dept1\", 1000),\n",
    "    (2, \"BBB\", \"dept1\", 1100),\n",
    "    (3, \"CCC\", \"dept1\", 3000),\n",
    "    (4, \"DDD\", \"dept1\", 5500),\n",
    "    (5, \"EEE\", \"dept2\", 8000),\n",
    "    (6, \"FFF\", \"dept2\", 9200),\n",
    "    (7, \"GGG\", \"dept3\", 1100),\n",
    "    (None, None, None, 5500),\n",
    "    (9, \"III\", None, 3500),\n",
    "    (10, None, \"dept5\", 2500)]\n",
    "\n",
    "dept = [(\"dept1\", \"Department - 1\"),\n",
    "        (\"dept2\", \"Department - 2\"),\n",
    "        (\"dept3\", \"Department - 3\"),\n",
    "        (\"dept4\", \"Department - 4\")\n",
    "       ]\n",
    "\n",
    "df = spark.createDataFrame(emp, [\"id\", \"name\", \"dept\", \"salary\"])\n",
    "deptdf = spark.createDataFrame(dept, [\"id\", \"name\"]) \n",
    "\n",
    "# Create Temp Tables\n",
    "df.createOrReplaceTempView(\"empdf\")\n",
    "deptdf.createOrReplaceTempView(\"deptdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Expresiones SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos usar la expresión SQL para la manipulación de datos. Tenemos la función **expr** y también una variante de un método de selección como **selectExpr** para la evaluación de expresiones SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+------+------------+\n",
      "|  id|name| dept|salary|salary_level|\n",
      "+----+----+-----+------+------------+\n",
      "|   1| AAA|dept1|  1000|  low_salary|\n",
      "|   2| BBB|dept1|  1100|  low_salary|\n",
      "|   3| CCC|dept1|  3000|  mid_salary|\n",
      "|   4| DDD|dept1|  5500| high_salary|\n",
      "|   5| EEE|dept2|  8000| high_salary|\n",
      "|   6| FFF|dept2|  9200| high_salary|\n",
      "|   7| GGG|dept3|  1100|  low_salary|\n",
      "|null|null| null|  5500| high_salary|\n",
      "|   9| III| null|  3500|  mid_salary|\n",
      "|  10|null|dept5|  2500|  mid_salary|\n",
      "+----+----+-----+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "cond = \"\"\"case when salary > 5000 then 'high_salary'\n",
    "               else case when salary > 2000 then 'mid_salary'\n",
    "                    else case when salary > 0 then 'low_salary'\n",
    "                         else 'invalid_salary'\n",
    "                              end\n",
    "                         end\n",
    "                end as salary_level\"\"\"\n",
    "\n",
    "newdf = df.withColumn(\"salary_level\", expr(cond))\n",
    "newdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando la función selectExpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+------+------------+\n",
      "|  id|name| dept|salary|salary_level|\n",
      "+----+----+-----+------+------------+\n",
      "|   1| AAA|dept1|  1000|  low_salary|\n",
      "|   2| BBB|dept1|  1100|  low_salary|\n",
      "|   3| CCC|dept1|  3000|  mid_salary|\n",
      "|   4| DDD|dept1|  5500| high_salary|\n",
      "|   5| EEE|dept2|  8000| high_salary|\n",
      "|   6| FFF|dept2|  9200| high_salary|\n",
      "|   7| GGG|dept3|  1100|  low_salary|\n",
      "|null|null| null|  5500| high_salary|\n",
      "|   9| III| null|  3500|  mid_salary|\n",
      "|  10|null|dept5|  2500|  mid_salary|\n",
      "+----+----+-----+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf = df.selectExpr(\"*\", cond)\n",
    "newdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones definidas por el usuario (UDF)\n",
    "A menudo necesitamos escribir la función en función de nuestro requisito muy específico. Aquí podemos aprovechar las udfs. Podemos escribir nuestras propias funciones en un lenguaje como python y registrar la función como udf, luego podemos usar la función para operaciones de DataFrame.\n",
    "\n",
    "* Función de Python para encontrar el nivel_salario para un salario dado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detSalary_Level(sal):\n",
    "    level = None\n",
    "\n",
    "    if(sal > 5000):\n",
    "        level = 'high_salary'\n",
    "    elif(sal > 2000):\n",
    "        level = 'mid_salary'\n",
    "    elif(sal > 0):\n",
    "        level = 'low_salary'\n",
    "    else:\n",
    "        level = 'invalid_salary'\n",
    "    return level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Luego registre la función \"detSalary_Level\" como UDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sal_level = udf(detSalary_Level, StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Aplicar función para determinar el salario_level para un salario dado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+------+------------+\n",
      "|  id|name| dept|salary|salary_level|\n",
      "+----+----+-----+------+------------+\n",
      "|   1| AAA|dept1|  1000|  low_salary|\n",
      "|   2| BBB|dept1|  1100|  low_salary|\n",
      "|   3| CCC|dept1|  3000|  mid_salary|\n",
      "|   4| DDD|dept1|  5500| high_salary|\n",
      "|   5| EEE|dept2|  8000| high_salary|\n",
      "|   6| FFF|dept2|  9200| high_salary|\n",
      "|   7| GGG|dept3|  1100|  low_salary|\n",
      "|null|null| null|  5500| high_salary|\n",
      "|   9| III| null|  3500|  mid_salary|\n",
      "|  10|null|dept5|  2500|  mid_salary|\n",
      "+----+----+-----+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf = df.withColumn(\"salary_level\", sal_level(\"salary\"))\n",
    "newdf.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
